{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "next_word_prediction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfKE3FP9zxaS"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from nltk.corpus import gutenberg\n",
        "import numpy as np\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3mS0XAhQmhd"
      },
      "source": [
        "**Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajeTcwxEEQOU",
        "outputId": "9b46b7ac-0442-42cd-8a90-94443d06a989"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('gutenberg')\n",
        "byrant=gutenberg.raw('bryant-stories.txt')\n",
        "whitman=gutenberg.raw('whitman-leaves.txt')\n",
        "burgass=gutenberg.raw('burgess-busterbrown.txt')\n",
        "chesteron=gutenberg.raw('chesterton-ball.txt')\n",
        "total_txt=byrant"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ8uu15mQxl9"
      },
      "source": [
        "**Text_preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpjDpn7vG5Kv"
      },
      "source": [
        "punc_removed=re.sub(r'\\W+', ' ', total_txt).lower()\n",
        "words = nltk.word_tokenize(punc_removed)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uM0JYST90yfl"
      },
      "source": [
        "for x in words :\n",
        "  if(len(x)<2 and x != 'a' and x != 'i'  ):\n",
        "    words.remove(x)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krItfO-YRggZ",
        "outputId": "705ed638-ed18-4d2a-84a0-5453b23071db"
      },
      "source": [
        "unique=list(set(words))\n",
        "len(unique)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3890"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lnGjZbMi6MrN"
      },
      "source": [
        "train_len = 4\n",
        "text_sequences = []\n",
        "for i in range(train_len,len(words)):\n",
        "  seq = words[i-train_len:i]\n",
        "  text_sequences.append(seq)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-O96wFIP01F"
      },
      "source": [
        "sequences = {}\n",
        "count = 1\n",
        "for i in range(len(words)):\n",
        "  if words[i] not in sequences:\n",
        "    sequences[words[i]] = count\n",
        "    count += 1"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBDxvVnc0nT5"
      },
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text_sequences)\n",
        "sequences = tokenizer.texts_to_sequences(text_sequences)\n",
        "vocabulary_size = len(tokenizer.word_counts)+1"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8cUrVQVFkUg"
      },
      "source": [
        "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
        "for i in range(len(sequences)):\n",
        "  n_sequences[i] = sequences[i]\n",
        "train_inputs = n_sequences[:,:-1]\n",
        "train_targets = n_sequences[:,-1]\n",
        "train_targets = to_categorical(train_targets, num_classes=vocabulary_size)\n",
        "seq_len = train_inputs.shape[1]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7vbi_qWQ-Zl"
      },
      "source": [
        "**Loading the tensorflow hub universal senetence encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gglBw5wWlMCD"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olkk5HIl4nCP"
      },
      "source": [
        "seq_dict=embed(unique)\n",
        "seq_dict=np.array(seq_dict)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUGomJHv-0dZ"
      },
      "source": [
        "embedding_layer =tf.keras.layers.Embedding(\n",
        "    seq_dict.shape[0],\n",
        "    512,\n",
        "    weights=[seq_dict],\n",
        "    input_length=seq_len,\n",
        "    trainable=False,\n",
        ")"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INjYBhVWYm7o"
      },
      "source": [
        "**`Model building and transfer learning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71ZAZH77D3tn",
        "outputId": "00aaa00f-3767-42f1-c97e-ffc5fc6c531c"
      },
      "source": [
        "input1 = tf.keras.Input(shape=seq_len,name=\"input1\")\n",
        "emb= embedding_layer(input1)\n",
        "bls1=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(256,return_sequences=True))(emb)\n",
        "bls2=tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128))(bls1)\n",
        "fc1=tf.keras.layers.Dense(64, activation='relu')(bls2)\n",
        "output=tf.keras.layers.Dense(vocabulary_size, activation='softmax')(fc1)\n",
        "model=tf.keras.models.Model(inputs=input1,outputs=output)\n",
        "print(model.summary())\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input1 (InputLayer)          [(None, 3)]               0         \n",
            "_________________________________________________________________\n",
            "embedding_1 (Embedding)      (None, 3, 512)            1991680   \n",
            "_________________________________________________________________\n",
            "bidirectional_2 (Bidirection (None, 3, 512)            1574912   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 256)               656384    \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 3890)              252850    \n",
            "=================================================================\n",
            "Total params: 4,492,274\n",
            "Trainable params: 2,500,594\n",
            "Non-trainable params: 1,991,680\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02snUWSZEL2j",
        "outputId": "478ffde7-43c8-41f1-bfd6-1e84e00cbcd6"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_inputs,train_targets,epochs=32,verbose=1)\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/32\n",
            "1450/1450 [==============================] - 17s 8ms/step - loss: 6.4076 - accuracy: 0.0784\n",
            "Epoch 2/32\n",
            "1450/1450 [==============================] - 12s 9ms/step - loss: 5.7390 - accuracy: 0.0986\n",
            "Epoch 3/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 5.4857 - accuracy: 0.1122\n",
            "Epoch 4/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 5.2606 - accuracy: 0.1182\n",
            "Epoch 5/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 5.0721 - accuracy: 0.1261\n",
            "Epoch 6/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 4.9114 - accuracy: 0.1344\n",
            "Epoch 7/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 4.7597 - accuracy: 0.1423\n",
            "Epoch 8/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 4.5982 - accuracy: 0.1528\n",
            "Epoch 9/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 4.4542 - accuracy: 0.1598\n",
            "Epoch 10/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 4.2919 - accuracy: 0.1730\n",
            "Epoch 11/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 4.0866 - accuracy: 0.1849\n",
            "Epoch 12/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 3.9134 - accuracy: 0.2028\n",
            "Epoch 13/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 3.7200 - accuracy: 0.2213\n",
            "Epoch 14/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 3.5204 - accuracy: 0.2436\n",
            "Epoch 15/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 3.3144 - accuracy: 0.2706\n",
            "Epoch 16/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 3.1098 - accuracy: 0.3010\n",
            "Epoch 17/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 2.9128 - accuracy: 0.3341\n",
            "Epoch 18/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 2.7042 - accuracy: 0.3731\n",
            "Epoch 19/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 2.5338 - accuracy: 0.4033\n",
            "Epoch 20/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 2.3387 - accuracy: 0.4376\n",
            "Epoch 21/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 2.1609 - accuracy: 0.4809\n",
            "Epoch 22/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 2.0183 - accuracy: 0.5098\n",
            "Epoch 23/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.8647 - accuracy: 0.5455\n",
            "Epoch 24/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.7317 - accuracy: 0.5727\n",
            "Epoch 25/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.5914 - accuracy: 0.6053\n",
            "Epoch 26/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.4744 - accuracy: 0.6341\n",
            "Epoch 27/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.3989 - accuracy: 0.6502\n",
            "Epoch 28/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.2964 - accuracy: 0.6749\n",
            "Epoch 29/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.2032 - accuracy: 0.6966\n",
            "Epoch 30/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.1518 - accuracy: 0.7047\n",
            "Epoch 31/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.0883 - accuracy: 0.7243\n",
            "Epoch 32/32\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 1.0152 - accuracy: 0.7417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbc0955e750>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpnVE9nVRwvl"
      },
      "source": [
        "emb.trainable=True"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qda3Q-N2Th9-",
        "outputId": "59c629cd-372d-44db-a732-cd292592880d"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(train_inputs,train_targets,epochs=16,verbose=1)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/16\n",
            "1450/1450 [==============================] - 16s 8ms/step - loss: 1.0188 - accuracy: 0.7377\n",
            "Epoch 2/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.9316 - accuracy: 0.7592\n",
            "Epoch 3/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.8837 - accuracy: 0.7721\n",
            "Epoch 4/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.8487 - accuracy: 0.7797\n",
            "Epoch 5/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.8104 - accuracy: 0.7858\n",
            "Epoch 6/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.7908 - accuracy: 0.7942\n",
            "Epoch 7/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.7695 - accuracy: 0.7993\n",
            "Epoch 8/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.7338 - accuracy: 0.8024\n",
            "Epoch 9/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.7193 - accuracy: 0.8082\n",
            "Epoch 10/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.7069 - accuracy: 0.8093\n",
            "Epoch 11/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.6732 - accuracy: 0.8183\n",
            "Epoch 12/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.6579 - accuracy: 0.8225\n",
            "Epoch 13/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.6545 - accuracy: 0.8231\n",
            "Epoch 14/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.6310 - accuracy: 0.8260\n",
            "Epoch 15/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.6168 - accuracy: 0.8313\n",
            "Epoch 16/16\n",
            "1450/1450 [==============================] - 12s 8ms/step - loss: 0.6105 - accuracy: 0.8307\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fbc08d9d350>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j38OVVVePrX1"
      },
      "source": [
        "model.save('./drive/MyDrive/saved_models/NWP_BILSTM.h5')\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNzLa6qxPfs0"
      },
      "source": [
        "**Test_demo**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fseN3S1RHew"
      },
      "source": [
        "model=tf.keras.models.load_model(\"./drive/MyDrive/saved_models/NWP_BILSTM.h5\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQtyAunaExh6",
        "outputId": "2cb1aaf9-ae65-4a94-91a5-ccf1fda0edf2"
      },
      "source": [
        "input_text = input().strip().lower()\n",
        "encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
        "pad_encoded = pad_sequences([encoded_text], maxlen=3, truncating='pre')\n",
        "print(encoded_text, pad_encoded)\n",
        "for i in (model.predict(pad_encoded)[0]).argsort()[-3:][::-1]:\n",
        "    pred_word = tokenizer.index_word[i]\n",
        "    print(\"Next word suggestion:\",pred_word)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the cake is\n",
            "[1, 897, 30] [[  1 897  30]]\n",
            "Next word suggestion: going\n",
            "Next word suggestion: as\n",
            "Next word suggestion: to\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}